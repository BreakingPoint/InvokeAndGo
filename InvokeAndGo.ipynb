{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# InvokeAndGo\n",
        "\n",
        "* This Notebook installs and runs a (temporary) instance of [InvokeAI](https://github.com/invoke-ai/InvokeAI) with a single click, accessable through its web frontend, using a photo realistic Stable Diffusion 1.5 model.\n",
        "\n",
        "* It is publically accessable through a [Localtunnel](https://theboroer.github.io/localtunnel-www/) in a protected cloud environment (Google Colab for example). At the end of the installation the console shows the link and password. After InvokeAI prints out the localhost HTTP address you can open the link to the tunnel and access the installation.\n",
        "\n",
        "* You need a runtime with [GPU support](https://cloud.google.com/compute/docs/gpus). As of now the installation even runs on the free-of-charge Google Colab T4 runtime! Installation takes around 10 minutes.\n",
        "\n",
        "* If you want to use a specific version of InvokeAI or a different set of models, just edit the configuration section at the beginning of the code. Since Google Colab does [not allow multiline inputs](https://colab.research.google.com/notebooks/forms.ipynb) (for a list of various models) I chose to keep it in pure code.\n",
        "  * If you want to speed the installation up you can remove the IP-Adapter and ControlNet models and load them as needed using the [model manager page](https://invoke-ai.github.io/InvokeAI/installation/050_INSTALLING_MODELS/#installation-via-the-web-gui) of InvokeAI's UI later.\n"
      ],
      "metadata": {
        "id": "etaI4xtZPlVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ------------------------------------------------ Configuration of the script:\n",
        "\n",
        "# InvokeAI version to install\n",
        "# https://github.com/invoke-ai/InvokeAI/tags\n",
        "cfg_invokeai_version = '3.6.3'\n",
        "\n",
        "# List of specific models needed for the task:\n",
        "# https://invoke-ai.github.io/InvokeAI/installation/050_INSTALLING_MODELS/#installation-via-invokeai-model-install\n",
        "cfg_models = [\n",
        "\n",
        "  # Base-Models:\n",
        "  'stablediffusionapi/realistic-vision-v51',\n",
        "    # https://huggingface.co/stablediffusionapi/realistic-vision-v51\n",
        "    # Recommended: \"UniPC\" Scheduler, 40 Steps, 6 CFG Scale\n",
        "\n",
        "  # IP-Adapters:\n",
        "  'InvokeAI/ip_adapter_sd_image_encoder',\n",
        "  'InvokeAI/ip_adapter_sd15',\n",
        "  'InvokeAI/ip_adapter_plus_sd15',\n",
        "    # https://huggingface.co/InvokeAI\n",
        "\n",
        "  # ControlNet:\n",
        "  'lllyasviel/sd-controlnet-canny',\n",
        "  'lllyasviel/sd-controlnet-hed',\n",
        "  'lllyasviel/sd-controlnet-scribble',\n",
        "    # https://huggingface.co/lllyasviel\n",
        "]\n",
        "\n",
        "# Set to false if you need to check the installation process and keep the info\n",
        "# on the screen:\n",
        "cfg_clear_install_log = True\n",
        "\n",
        "\n",
        "## -------------------------------- Install InvokeAI and run web frontend after:\n",
        "\n",
        "import os\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Prepare destination folder:\n",
        "rootfolder = '/content/invokeai'\n",
        "outfolder = f\"{rootfolder}/output\"\n",
        "\n",
        "os.makedirs(outfolder)\n",
        "os.environ['INVOKEAI_ROOT'] = rootfolder\n",
        "\n",
        "%cd {rootfolder}\n",
        "\n",
        "## ----------------------------------------------------------- Install InvokeAI:\n",
        "\n",
        "# https://pypi.org/project/InvokeAI/\n",
        "!pip install InvokeAI[xformers]=={cfg_invokeai_version} --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "## Configure InvokeAI without default models to save time and resources:\n",
        "\n",
        "# https://github.com/invoke-ai/InvokeAI/blob/main/docs/features/UTILITIES.md#invokeai-configure\n",
        "!invokeai-configure --yes --default_only --skip-sd-weights\n",
        "\n",
        "## ------------------------------------------------------------- Install models:\n",
        "\n",
        "for model in cfg_models:\n",
        "  !invokeai-model-install --add {model}\n",
        "\n",
        "if cfg_clear_install_log:\n",
        "  clear_output()\n",
        "\n",
        "## ---------------------------------- Install and run tunnel (as separate task):\n",
        "## for public HTTP access to InvokeAI's localhost web frontend:\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(\"After start of the InvokeAI webserver open the following link:\")\n",
        "print()\n",
        "!npm install -g localtunnel\n",
        "!nohup npx localtunnel --port 9090 &\n",
        "time.sleep(5)\n",
        "  #todo: scan for HTTP address\n",
        "!cat {rootfolder}/nohup.out\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(\"Use this IP address as password for the tunnel:\")\n",
        "print()\n",
        "!curl ipv4.icanhazip.com\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(\"Starting InvokeAI webserver. Wait for the localhost link in the console\")\n",
        "print(\"before clicking on the tunnel link to run the web frontend.\")\n",
        "print()\n",
        "\n",
        "## ----------------------------------------------------------- Run web frontend:\n",
        "\n",
        "# https://invoke-ai.github.io/InvokeAI/features/WEB/\n",
        "!invokeai-web --outdir={outfolder}\n"
      ],
      "metadata": {
        "id": "NO3XyDPsTJ2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
